This repository is for collecting data regarding the digital vitality of languages listed in iso-693-3.
The data collected get stored in an sqlite database, its schema can be seen in dld/models.py .
For the several data sources there are different parsers in extern/ld/parsers; some of these 
download the data they process, some need input data which was downloded before, and some can work both in online
and offline mode. There is a script (extern/ld/parser_aggregator.py) for running all parsers and
merging their outputs into a table (langdeath.db.sqlite).

The repository also contains scripts for performing machine learning experiments for
classifying the languages according to their digital vitality (based on the collected data),
using pandas and sklearn. This is a supervised learning method with the SIL codes of the languages
used as training data listed in classifier/seed_data.
The classifier.py script performs a set of experiments, training models of randomly sampled
part of the training data, and classifying all the languages into the categories
still, historic, vital, thriving and global.


Usage 

0. In the langdeath folder:
    pip install -U .
    pip install -e .
    export DJANGO_SETTINGS_MODULE=langdeath.settings

0.2 Getting the data
    For the parsers, that use offline data, these have to be put in a common directory, and the default filenames where the parser_aggregator
    looks for them are listed in extern/ld/res/dump_filenames.
    (If these files/directories are missing, the parser_aggregator still runs, but skips these sources.)

    The listed filenames:
    -Ethnologue_Table_of_Languages.tab
    
    -olac_language_archives_20151212: html sites of http://www.language-archives.org/language/${LANGUAGE_CODE}
        If it is not given, the parser itself downloads the data
    
    -DBPedia_201510/: containing the following files:
                            instance-types_en.nt
                            raw_infobox_properties_en.nt
                            mappingbased_properties_cleaned_en.nt
                            short_abstracts_en.nt
                            dbpedia_ontology_languages:  
                            (this can be generated from instance-types_en.nt,
                        by taking the lines with "<http://dbpedia.org/ontology\/Language\> ." as last column) 

    -WP_20151209_wikiextractor: containing all wikipedia dumps, from
    https://dumps.wikimedia.org/${WIKI_CODE}wiki/latest/${WIKI_CODE}wiki-latest-pages-articles.xml.bz2
    extracted using WikiExtractor
    (see http://attardi.github.io/wikiextractor/)

    - : ,a wikipedia incubator dump, extracted using WikiExtractor.
    
    -endangered_html_20151211_csv20160114: html sites of http://www.endangeredlanguages.com/lang/${LANGUAGE_CODE}
        If empty, the parser itself downloads the data
    uriel_v0_2_features_avg.csv: 


1. Prior to running the parsers:
    python manage.py syncdb
    python extern/ld/load_country_data.py extern/ld/res/country_alt_names
    

2. To run the parsers
    python expern/ld/parser_aggregator.py -d DATA_DUMP_DIR -p PICKLE_DIR -l LOG_DIR -f FILENAME_MAPPINGS 
   
    (run python expern/ld/parser_aggregator.py --help for listing the options)
    This will call all the parsers, which collect the data in a langdeath.db.sqlite database.
    This should take a couple of minutes for all parsers, except for the parsers which process dbpedia and wikipedia dumps.
    The options for the parser_aggregator (see python parser_aggregator --help):
    
    DATA_DUMP_DIR: Dump files (listed in 0.2 are to be put here.)
    FILENAME_MAPPING: Default is extern/ld/res/dump filenames
    PICKLE_DIR: The output of the more slower parsers get saved, so when re-running it only reads the data from here. These are simple lists of python
    dictionaries. 
    LOG_DIR:  For every parser there will be a ${ParserClass}.found, ${ParserClass}.not_found file produced
              containing the languages produced by the parsers which could or could not be merged to an SIL language (based on the language code or name
              parsed). For those parsers which produce alternative names to some languages, these are listed in a ${ParserClass}.altnames file.


3. To export the data into tsv, run python preprocess.py from the classifier directory.
   (see python preprocess.py --help)

4. To perform the classification experiments using the exported data, run python classifier.py from the classifier directory. 
   (see python classifier.py --help for listing the options)
   After 

